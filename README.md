The project is a full-fledged Multimodal Retrieval-Augmented Generation (RAG) system which can respond to user queries depending on multiple sources of data like CSV files, PDFs, scanned images, and Wikipedia articles. The process begins with data ingestion and preprocessing: images get passed through OCR via EasyOCR, customer churn CSVs' tabular data get cleaned and converted, PyMuPDF is utilized to parse PDFs, and Wikipedia topics associated with them get extracted. All text content is standardized and segmented into manageable chunks. These chunks are then embedded with the `all-MiniLM-L6-v2` sentence transformer and indexed using FAISS for efficient similarity search. When a user poses a question, the system retrieves the most semantically similar chunks based on cosine similarity and generates a context-rich prompt.
This query is then fed to the Mistral-7 B-Instruct language model to generate well-formed, context-sensitive responses. For semantic similarity with source text calculation when comparing generated responses, models like `multilingual-e5-base` are used. The model supports well-structured and unstructured content quite effectively so that customer analytics use cases like churn analysis can benefit from good question-answering. Modularity and multimodal capability of the pipeline make it suitable for application in business applications of knowledge retrieval, customer service automation, and decision support systems. The model has attained a cosine similarity of 91%. 
